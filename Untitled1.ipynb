{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c1750-ef26-4e70-bbc8-2cb9a9b41e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.text import Text\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9f43d-7345-40c2-8388-47663d7d01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7497325-c90e-40d2-92aa-ade733846b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Twitter Hate Speech.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c67070f-af44-4590-9f9b-2948a653abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187dca00-72ff-436f-b296-76295b151889",
   "metadata": {},
   "outputs": [],
   "source": [
    "##data information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc8a63-de82-492d-9823-7780cf7fd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of observation:{df.shape[0]}')\n",
    "print(f'number of features:{df.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1027a-9229-40cd-8691-af7afde98ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b881ac4-7ef2-4497-98b3-a035c2eaec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365f0ae-9b62-4db3-ada6-ebe6966e1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052dd90f-214d-498e-88b0-eab381639823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f086c-daee-44db-9004-a631fb8d933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.iloc[:, 1:]\n",
    "text.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca58a9b-92ff-4ef3-9a86-76692cc9a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df.iloc[:, 0:1]\n",
    "label.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34459e9-5da7-4072-8da4-88f6c52e09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3bc19b-38b1-47af-ad7a-6ac89d3575cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_speech = df[df['label'] == 1].shape[0]\n",
    "free_speech = df[df['label'] == 0].shape[0]\n",
    "print('Hate Speech =', hate_speech)\n",
    "print('Free Speech =', free_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280cfbf8-c738-4bfd-9bdd-deca161de3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = [hate_speech, free_speech]\n",
    "label = [\"Hate Speech\", \"Free Speech\"]\n",
    "\n",
    "plt.pie(speech, labels = label, shadow = False, wedgeprops = {'edgecolor': 'black'}, \n",
    "        autopct = '%1.1f%%', startangle= 90, colors=['red', 'blue'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d87fcd6-d745-4f8f-a861-90afbe779ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c5463-5f50-430c-accf-88bc812f4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf510a-52af-4ae5-91ce-6b17b6e38ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    return text.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f49126-d99f-4ede-ac93-b738cfe0b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tag(text):\n",
    "    newtext= re.sub(r'(@[A-Za-z0-9]+)',\"\",text)\n",
    "    return newtext\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(remove_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3d025-9745-4cf2-96b7-8ea762902ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special(text):\n",
    "    return \" \".join(e for e in text.split() if e.isalnum())\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(remove_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427646cd-fedc-4c02-8e39-f1e51cfdca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs from a text string.\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b4f09-6354-4c5c-9341-9b6f56175234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04bde8a-6b56-41ad-ad12-0b3fc9b882e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266216c-6c97-463f-8d3b-d21ff177e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f926b-cb4c-4dd6-919a-7b4035638e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b498b325-a2f9-456f-ae50-b067ee861ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae6815-4ae1-4f08-bf0a-c1676d4d681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46383e90-e034-4b46-afe9-1cc6db158f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(text):\n",
    "    text  = [i for i in text if not i in stopwords.words('english')]\n",
    "    return text\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(remove_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475f857-4389-43ab-868f-4df67dd81c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478e500-e6bd-4920-9db7-1e44317fb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5243c-ac3b-475c-8fa4-69bb115090bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmatize(text):\n",
    "    word_lem = WordNetLemmatizer()\n",
    "    text = [word_lem.lemmatize(token) for token in text]\n",
    "    return text\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(Lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da7624-7537-4839-8dad-2797719199cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971fbf2-73e7-483c-a206-f14547fb153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab70d3dc-f99e-4560-b782-507d1fa87155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud (hate speech)\n",
    "hate_speech = df[df['label'] == 1]   \n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "for val in hate_speech.tweet:\n",
    "     \n",
    "    #typecaste \n",
    "    val = str(val)\n",
    " \n",
    "    #split\n",
    "    tokens = val.split()\n",
    "     \n",
    "    #Converts each token \n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    " \n",
    "# plot                     \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed789076-0a5a-48e9-8809-30bc04c1ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud ( free speech)\n",
    "free_speech = df[df['label'] == 0]   \n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "for val in free_speech.tweet:\n",
    "     \n",
    "    #typecaste\n",
    "    val = str(val)\n",
    " \n",
    "    #split the value\n",
    "    tokens = val.split()\n",
    "     \n",
    "    #Converts each token \n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    " \n",
    "# plot               \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fecc2-3519-4f87-a635-39c0f116cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ed48c-032c-4b43-a76b-4bc6ebbed0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e3890-716b-4f2c-a723-4a148e271b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features= 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ab2a2-6477-4fe2-969c-521063140f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_str = []\n",
    "for lists in df['tweet']:\n",
    "    list_to_str.append(' '.join(map(str, lists)))\n",
    "\n",
    "df['tweet'] = list_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22afc07a-21c3-478e-9281-3049687efc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['tweet']\n",
    "text = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6abed-a482-496e-81bb-76133df763ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609c244-505f-48ed-9cd8-b42c54150e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72fc4c-d9af-4475-bb1d-86bd4cea2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68bfa7e-79b8-4613-831d-86b8b960690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df.iloc[:, 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bca570-3586-4995-a8dc-67deef0e22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c6dcc-65a5-4630-94a2-4398ff0e921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca455f7-1f1e-446a-8bce-dbdc5a825126",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Convolutional Neural Netowrk with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442cec2b-d20b-4cdc-87ba-129123d531b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Bidirectional, TimeDistributed, Conv1D, ZeroPadding1D, GRU\n",
    "from tensorflow.keras.layers import Lambda, Input, Dropout, Masking, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98739cc-d9de-4138-8103-3cf6200d6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.python import keras\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Flatten\n",
    "#from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape, Merge\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
    "\n",
    "# CNN + LSTM Model\n",
    "def cnn_lstm(input_dim, output_dim, dropout=0.2, n_layers=1):\n",
    " \n",
    "    # Input data type\n",
    "    dtype = 'float32'\n",
    " \n",
    "    # ---- Network model ----\n",
    "    input_data = Input(name='the_input', shape=input_dim, dtype=dtype)\n",
    "     # Assuming input_dim is a tuple with at least two elements (height, width)\n",
    "   \n",
    "\n",
    "# Reshape input to add an extra dimension\n",
    "    \n",
    "    # Reshape input to add an extra dimension\n",
    "  # x = Reshape((input_dim[0], input_dim[1], 1))(input_data)\n",
    " # Assuming input_dim is a tuple with at least two elements (height, width)\n",
    "    height, width = input_dim\n",
    "\n",
    "# Reshape input to add an extra dimension\n",
    "    x = Reshape((height, width, 1))(input_data)\n",
    "\n",
    "    # 1 x 1D convolutional layers with strides 4\n",
    "    x = Conv1D(filters=256, kernel_size=10, strides=4, name='conv_1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout, name='dropout_1')(x)\n",
    "        \n",
    "    x = LSTM(128, activation='relu', return_sequences=True,\n",
    "             dropout=dropout, name='lstm_1')(x)\n",
    "    x = LSTM(128, activation='relu', return_sequences=False,\n",
    "              dropout=dropout, name='lstm_2')(x)\n",
    "    \n",
    "    x = Dense(units=64, activation='relu', name='fc')(x)\n",
    "    x = Dropout(dropout, name='dropout_2')(x)\n",
    " \n",
    "    # Output layer with softmax\n",
    "    y_pred = Dense(units=output_dim, activation='sigmoid', name='sigmoid')(x)\n",
    " \n",
    "    network_model = Model(inputs=input_data, outputs=y_pred)\n",
    "    \n",
    "    return network_model\n",
    "    num_samples, height, width = X_train.shape\n",
    "    model = cnn_lstm(X_train.shape[1:], 1)\n",
    "    \n",
    "  #  model = cnn_lstm((height, width), 1)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam, metrics= 'accuracy')\n",
    "    model.fit(\n",
    "    \n",
    "        X_train, y_train,\n",
    "        batch_size=128,\n",
    "        epochs=25,\n",
    "        validation_data=(X_test, y_test),\n",
    ")\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e61786-9817-4d06-9794-fdc2f716a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b6b47-5288-432b-b141-2ed566e3ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "#[row, column]\n",
    "TP = confusion_matrix[1, 1]        \n",
    "TN = confusion_matrix[0, 0]           \n",
    "FP = confusion_matrix[0, 1]           \n",
    "FN = confusion_matrix[1, 0]\n",
    "\n",
    "\n",
    "# Visualize the Matrix\n",
    "group_names = ['TN','FP','FN','TP']\n",
    "\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in confusion_matrix.flatten()]\n",
    "\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in confusion_matrix.flatten()/np.sum(confusion_matrix)]\n",
    "\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "sns.heatmap(confusion_matrix, annot=labels, fmt='', cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1ebb1e-db2d-46b0-b76c-954c3cc90754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023d43d-64a6-40eb-a340-814b8cc3ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Score\n",
    "Accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print('Accuracy Score:', Accuracy) \n",
    "\n",
    "# Precision Score\n",
    "Precision = precision_score(y_test, y_pred_test)\n",
    "print('Precision Score:', Precision)   \n",
    "\n",
    "# True positive Rate (TPR) or Sensitivity or Recall\n",
    "TPR = recall_score(y_test, y_pred_test)\n",
    "print('True positive Rate:', TPR)             \n",
    "\n",
    "# False positive Rate (FPR)\n",
    "FPR = FP / float(TN + FP)\n",
    "print('False positive Rate', FPR)                       \n",
    "\n",
    "# F1 Score or F-Measure or F-Score\n",
    "F1 = f1_score(y_test, y_pred_test)\n",
    "print('F1 Score:', F1)                 \n",
    "\n",
    "# Specificity\n",
    "Specificity = TN / (TN + FP)\n",
    "print('Specificity:', Specificity )                    \n",
    "\n",
    "# Mean Absolute Error\n",
    "Error = mean_absolute_error(y_test, y_pred_test)\n",
    "print('Mean Absolute Error:', Error)   \n",
    "\n",
    "# ROC Area\n",
    "Roc = roc_auc_score(y_test, y_pred_test)\n",
    "print('ROC Area:', Roc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2848bb1-83a4-4376-acf8-878a3c8917bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "result = [Accuracy, Precision, TPR, FPR, F1, Specificity, Error, Roc]\n",
    "label = [\"Accuracy\", \"Precision\", \"TPR\", \"FPR\", \"F-Score\", \"Specificity\", \"Error\", \"Roc Area\"]\n",
    "colors=[ 'red', 'green', 'blue', 'darkgoldenrod', 'orange', 'purple', 'brown', 'darkcyan']\n",
    "\n",
    "plt.bar(label, result, color = colors, edgecolor='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd726d-a965-4a85-8daa-dba60c65f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93d40d-76db-44e2-ae5c-c3357e5a0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF= open(\"cv.pkl\",\"wb\")          \n",
    "pickle.dump(vectorizer,TF_IDF)                                  \n",
    "TF_IDF.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d85e6-ca65-4f7a-815e-0226f640f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm = open(\"model.pkl\",\"wb\")          \n",
    "pickle.dump(model,cnn_lstm)                                  \n",
    "cnn_lstm.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1276d8-0fad-4cb4-b84a-f717e2a23608",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = open(\"cv.pkl\",\"rb\")           \n",
    "cv = pickle.load(cv)                                 \n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ec36f-613e-41c9-b7d5-89d1697455f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = open(\"model.pkl\",\"rb\")           \n",
    "model = pickle.load(model)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824b4f5-0495-44ff-ab88-ae4b22950b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive News example\n",
    "comment = [\"I support racism. I don't care\"]\n",
    "vect = cv.transform(comment).toarray()\n",
    "model.predict(vect)\n",
    "\n",
    "if model.predict(vect) == 1:\n",
    "    print(\"Hate Speech\")\n",
    "else:\n",
    "    print(\"Free Speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2239d38-19d8-491c-9224-813ccb7f6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive News example\n",
    "comment = [\" I respect the all kind of nationalities\"]\n",
    "vect = cv.transform(comment).toarray()\n",
    "model.predict(vect)\n",
    "\n",
    "if model.predict(vect) == 1:\n",
    "    print(\"Hate Speech\")\n",
    "else:\n",
    "    print(\"Free Speech\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
